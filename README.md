# Awesome Programming (Generative) Pixel Art



## 24×24 Pixel Punk Heads - Text-To-Image "No A.I." (Spritesheet) Models / Generators

> Punk • Female Light • Wild Blonde  • Blue Eye Shadow  • Hot Lipstick  • Mole

[**Inside the Billion Dollar $$$ (Crypto) Punk Pixel Heads**](
https://github.com/cryptopunksnotdead/cryptopunks/tree/master/insidepunks)






## Future Aside - Text-To-Image A.I. (Diffusion) Models / Generators

> An astronaut riding a horse in space in the style of Andy Warhol.


<details>
<summary>More</summary>

or

> A painting of a fox sitting in a field at sunrise in the style of Claude Monet.

or
  
> A small cactus wearing a straw hat and neon sunglasses in the Sahara desert.

or

> A blue jay standing on a large basket of rainbow macarons.


</details>



**(Open A.I.) Dalle E·2** @ <https://openai.com/dall-e-2/>

> DALL·E 2 is a new AI system that can create realistic images and art from a description in natural language.
>
> DALL·E 2 can create original, realistic images and art 
> from a text description. It can combine concepts, attributes, and styles.


**(Google) Imagen** @ <https://imagen.research.google/>

> Imagen is an AI system that creates photorealistic images from input text.
>
> Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism 
> and a deep level of language understanding. 
> Imagen builds on the power of large transformer language models in understanding text 
> and hinges on the strength of diffusion models in high-fidelity image generation. 
>
>  Imagen uses a large frozen T5-XXL encoder to encode the input text into embeddings. 
> A conditional diffusion model maps the text embedding into a 64×64 image. 
> Imagen further utilizes text-conditional super-resolution diffusion models to upsample the image 64×64→256×256 and 256×256→1024×1024.


**(Stability AI) Stable Diffusion** @ <https://stability.ai/blog/stable-diffusion-public-release>
 
> Given a text prompt, Stable Diffusion can generate photorealistic 512×512 pixel images depicting the scene described in the prompt.



More (Text-To-Image) Models:

Trained on COCO¹

- AttnGAN (Xu et al., 2017)	35.49 COCO FID²
- DM-GAN (Zhu et al., 2019)	32.64 -
- DF-GAN (Tao et al., 2020)	21.42 -
- DM-GAN + CL (Ye et al., 2021)	20.79 -
- XMC-GAN (Zhang et al., 2021)	9.33  -
- LAFITE (Zhou et al., 2021)	8.12
- Make-A-Scene (Gafni et al., 2022)	7.55

Not trained on COCO

- DALL-E (Ramesh et al., 2021)	17.89  COCO FID
- GLIDE (Nichol et al., 2021)	12.24
- DALL-E 2 (Ramesh et al., 2022)	10.39


¹: What is (Microsoft) COCO?

<https://cocodataset.org/>,
<https://github.com/cocodataset/cocodataset.github.io> - COCO is a large-scale object detection, segmentation, and captioning dataset. COCO has several features:

- Object segmentation
- Recognition in context
- Superpixel stuff segmentation
- 330K images (>200K labeled)
- 1.5 million object instances
- 80 object categories
- 91 stuff categories
- 5 captions per image
- 250,000 people with keypoints


²: The Fréchet inception distance (FID) 
is a metric used to assess the quality of images created by a generative model, 
like a generative adversarial network (GAN).
Unlike the earlier inception score (IS), which evaluates only the distribution of generated images, 
the FID compares the distribution of generated images with the distribution 
of a set of real images ("ground truth").  - <https://en.wikipedia.org/wiki/Fréchet_inception_distance>





